#Building Pipeline
1. Create a Github and Clone it in local (Add Experiments)

___________________mongodb setup______________________________________
2. Sign up to mongodb atlas and create a new project by just providing it a name and then next next create
3. Organization-> project-> Cluster
3. From "create a cluster" screen hit "create",select mo service keeping other service as default , hit create "deployment"
4. Set up user name and password and then create db user->close
user_name=vaishnavibarolia_db_user
password=rDpnzMgseYw6bdnA
5. Go to network access and click on  add  ip address -"0.0.0.0/0" so that we can access it from anywhere and click allow access from anywhere and then click confirm 
6. Go back to project click on Get Connection String and select drivers->> python 3.6 or later version
---> copy and save the connection string 
----connection string= mongodb+srv://vaishnavibarolia_db_user:<db_password>@cluster0.b5qy2gy.mongodb.net/?appName=Cluster0
7. Add mongodb_setup.ipynb file to experiments and push your data to mongodb
8. Go to mongodb atlas >> Data base>> browse collection >> see your data in key value pair 
_______________mongodb setup completed____________________________________

9. Create src folder along with all components (run individually)
10. Add Data, models, reports directories to gitignore file
git add, commit, push

Setting up dvc pipeline without params
5.Create dvc.yaml file and add stages to it
6. dvc init then do dvc repro to test the pipeline automation (Check dvc dag) or apply dvc repro --force
6. git add commit push

Setting up dvc pipeline with parameters 
8.Add params.yaml file
9. Add the params setup(mention below)
10. Do dvc repro again 
11. Now add git, add, commit , push



ADD params 
def load_params(params_path:str)->dict:
    "Load params from params.yaml file"
    try:
        with open(params_path,"r") as f:
            params=yaml.safe_load(f)
        logger.debug("Parameters retrieved %s",params_path)
        return params
    except FileNotFoundError as e:
        logger.error("File not found %s",e)
        raise
    except yaml.YAMLError as e:
        logger.error("Yaml error: %s",e)
        raise
    except Exception as e:
        logger.error("Unexpected error occurred:%s",e)
        raise

Add to main

Experiments with dvc 
12 pip install dvclive
13 Add dvc live code blcock(mention below)
14 Do "dvc exp run" it will create a new dvc.yaml (if laready not there) and dvc directory (each run will be considered as an experiments)
15 Do "dvc exp show" on terminal to see the experiments or use extension on vscode
16 Do dvc exp remove {exp_name} to remove exp (optional) "dvc exp apply" {exp_name} to reproduce prev exp
17 Change params re-run code and (produce experiments)
18 Now add git, add, commit ,push




live.log_params(param)
3. dvc exp run

Adding a console s3 storage to DVC:
1. Login to AWS console
2. Create an IAM user 
3. Create S3 (enter unique name and create)
4. pip install dvc s3
5. pip install awscii
6 aws configure
7. dvc remote add -d dvcstore s3://bucketname
8 dvc commit-push the exp outcome that you want to keep
9 Finally git add commit push


_----------------------------------------

Place fastapp for fastapi and streamlit for frontend and then Dockerize
Create docker file for fastapi and streamlit individually 

docker build -t vaishnavibarolia/comment_toxicity:fastapi -f fastapp/Dockerfile .
docker build -t vaishnavibarolia/comment_toxicity:streamlit -f frontend/Dockerfile .
connect fastapp dockerfile and streamlit dockerfile with docker-compose.yaml
docker-compose yamlcode:
with services:
  fastapi:
    build:
      context: .
      dockerfile: fastapp/Dockerfile
    container_name: fastapi_app
    ports:
      - "8000:8000"
    networks:
      - app_net

  streamlit:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    container_name: streamlit_app
    ports:
      - "8501:8501"
    depends_on:
      - fastapi
    networks:
      - app_net

networks:
  app_net:

  write docker-compose up --build in terminal to connect fastapp dockerfile and streamlit docker and see the reault in local host
  and in streamlit frontend give url as " http//:fastapi:8000/predict"

  docker run -p 8000:8000 vaishnavibarolia/comment_toxicity:fastapi
  
  docker run -p 8501:8501 vaishnavibarolia/comment_toxicity:streamlit

push to dcker hub

docker push vaishnavibarolia/comment_toxicity:fastapi
docker push vaishnavibarolia/comment_toxicity:streamlit

